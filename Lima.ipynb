{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# OpenStreetMap Data Wrangling, Lima (Peru) \n",
    "### Udacity's Data Analysis Nanodegree \n",
    "\n",
    "This project is part of [Udacity's Data Analysis Nanodegree](https://eu.udacity.com/course/data-analyst-nanodegree--nd002)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Map Area\n",
    "\n",
    "**Lima, Peru**\n",
    "\n",
    "Extraction Date: _2018 October 18th_\n",
    "\n",
    "* Extracted with Overpass-API from: https://overpass-api.de/api/map?bbox=-77.2693,-12.2784,-76.8996,-11.8577\n",
    "\n",
    "I lived most of 2017 in Lima, and one of the first things that surprised me the most when I arrived was the laxitude of the people. I selected Lima for being a city I know and I'm fond of; but mostly because of the challenge that this laxitude could imply in a collaboratively created dataset like OpenStreetMap."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Auditing the Data\n",
    "\n",
    "### File Size\n",
    "\n",
    "The dowloaded file is: Lima.osm ....... 232MB "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_PATH = \"Lima.osm\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XML Elements \n",
    "\n",
    "We will use XML Iterative Parsing to extract the list of tag names in the data file and how many of each:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bounds': 1,\n",
      " 'member': 69378,\n",
      " 'meta': 1,\n",
      " 'nd': 1177099,\n",
      " 'node': 936560,\n",
      " 'note': 1,\n",
      " 'osm': 1,\n",
      " 'relation': 2080,\n",
      " 'tag': 676118,\n",
      " 'way': 183810}\n"
     ]
    }
   ],
   "source": [
    "import xml.etree.cElementTree as ET\n",
    "from collections import defaultdict\n",
    "import pprint\n",
    "\n",
    "\n",
    "# Count XML element tags\n",
    "def count_tags(filename):\n",
    "    tags = defaultdict(int)\n",
    "    with open(filename,  'r', encoding=\"utf8\") as file:\n",
    "        context = ET.iterparse(file)\n",
    "        for event, elem in context:\n",
    "            tags[elem.tag] += 1\n",
    "            elem.clear()\n",
    "    return tags\n",
    "\n",
    "tags = count_tags(FILE_PATH)\n",
    "pprint.pprint(dict(tags))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the documentation in http://wiki.openstreetmap.org/wiki/OSM_XML:\n",
    "- osm: contains the version of the API and the generator used. In our case **version is 0.6** and **generator Overpass API 0.7.55.4**. \n",
    "- bounds: shows the minimum and maximum longitude and latitude of our map dataset.\n",
    "- node: single points in space that contain a set of tags as a key-value pair.\n",
    "    - tag: key-value pair.\n",
    "- way: it is a ordered list of nodes that normally contains a tag. Examples of ways could be roads and highways. \n",
    "    - nd: item of a way, referencing a node.\n",
    "    - tag: key-value pair.\n",
    "- relation: it is also a ordered list of nodes, ways or relations as members, with one or more tags. They are used to model relationships between elements \n",
    "    - member: item of a relation referencing a node, way or relation.\n",
    "    - tag: key-value pair."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tag Key Types\n",
    "\n",
    "As referenced in the Openstreetmap documentation, the information about the nodes, ways and relations is in the tag elements. Here is a sample node element:\n",
    "\n",
    "      <node id=\"3581929393\" lat=\"-12.1253878\" lon=\"-77.0306079\" version=\"8\" timestamp=\"2018-10-08T20:11:00Z\" changeset=\"63324048\" uid=\"309382\" user=\"Diego Sanguinetti\">\n",
    "    <tag k=\"addr:city\" v=\"Miraflores\"/>\n",
    "    <tag k=\"addr:housenumber\" v=\"399\"/>\n",
    "    <tag k=\"addr:street\" v=\"Calle San Martín\"/>\n",
    "    <tag k=\"amenity\" v=\"restaurant\"/>\n",
    "    <tag k=\"cuisine\" v=\"sushi;japanese;marina\"/>\n",
    "    <tag k=\"name\" v=\"Maido\"/>\n",
    "    <tag k=\"opening_hours\" v=\"Mo-Sa 12:30-22:45; Su 12:30-17:00\"/>\n",
    "    <tag k=\"phone\" v=\"+51 1 3135100\"/>\n",
    "    <tag k=\"toilets:wheelchair\" v=\"yes\"/>\n",
    "    <tag k=\"website\" v=\"http://maido.pe/\"/>\n",
    "    <tag k=\"wheelchair\" v=\"yes\"/>\n",
    "    <tag k=\"wheelchair:description\" v=\"Por telefono dicen: Que el restaurante el salon de comida se encuentra en el 2do piso. Hay elevador, a veces no funciona. El baño para silla esta en el primer piso.\"/>\n",
    "      </node>\n",
    "\n",
    "As can be seen, the tag keys are written in lower case, with lower dash instead of space and colon to indicate subkeys classification depending on the information to be provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exploring the keys in tags\n",
    "import re\n",
    "\n",
    "lower = re.compile(r'^([a-z]|_)*$')\n",
    "lower_colon = re.compile(r'^([a-z]|_)*:([a-z]|_)*$')\n",
    "problemchars = re.compile(r'[=\\+/&<>;\\'\"\\?%#$@\\,\\. \\t\\r\\n]')\n",
    "\n",
    "# Classify tags by character and structure type with regular expressions\n",
    "def get_tag_key_types(filename):\n",
    "    keys = {\"lower\" : set(), \"lower_colon\" : set(), \"problemchars\": set(), \"other\" : set()}\n",
    "    for _, elem in ET.iterparse(filename):\n",
    "        if elem.tag == \"tag\":\n",
    "                k = elem.attrib['k']    \n",
    "                if lower.match(k):\n",
    "                    keys['lower'].add(k)\n",
    "                elif lower_colon.match(k):\n",
    "                    keys['lower_colon'].add(k)\n",
    "                elif problemchars.search(k):\n",
    "                    keys['problemchars'].add(k)\n",
    "                else:\n",
    "                    keys['other'].add(k)\n",
    "        elem.clear()\n",
    "    return keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Different tag keys with lowecase: 325\n",
      "Different tag keys with lowercase and colon: 421\n",
      "Different tag keys with problem chars: 0\n",
      "Other different tag keys: 136\n"
     ]
    }
   ],
   "source": [
    "tag_keys = get_tag_key_types(FILE_PATH)\n",
    "print ('Different tag keys with lowecase:' , len(tag_keys['lower']))\n",
    "print ('Different tag keys with lowercase and colon:' , len(tag_keys['lower_colon']))\n",
    "print ('Different tag keys with problem chars:' , len(tag_keys['problemchars']))\n",
    "print ('Other different tag keys:' , len(tag_keys['other']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is no keys in our data that with problematic characters. However, auditing the \"other\" category we can find the following situations: \n",
    "- Tag keys with numbers. Example: 'fuel:octane_84',  'fuel:octane_87', 'fuel:octane_90', etc...\n",
    "- Tag keys with capital letters, like 'FIXME' or 'Source'. \n",
    "- Tag keys including initials or abbreviations, like 'ISO' or 'currency:PER'; which are correctly written. \n",
    "- Tag keys with more than one colon character, which seems to be used to add more specific information, like 'seamark:light:category' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unique contributing users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extracting unique users\n",
    "def get_unique_users(filename):\n",
    "    users = set()\n",
    "    for _, elem in ET.iterparse(filename):\n",
    "        if 'user' in elem.attrib:\n",
    "            users.add(elem.attrib['user'])\n",
    "    return users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_users = get_unique_users(FILE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique users contributing to the map data: 1882\n"
     ]
    }
   ],
   "source": [
    "print ('Number of unique users contributing to the map data:', len(unique_users))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Street types\n",
    "\n",
    "Now we are going to analyze the tags with key _addr:street_ to check the consistency and any problem this dataset might have. \n",
    "\n",
    "First we are going to define possible street types. In Spanish speaking countries we have to take into account that street name types are in the beginning of the name and not at the end. Through consecutive itterations we will add street types that weren't considered in first stages. \n",
    "\n",
    "A preliminar list was composed of the following street types: \n",
    "- Avenida\n",
    "- Calle\n",
    "- Óvalo\n",
    "- Malecón\n",
    "- Jirón"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "expected_st_names = [\"Avenida\", \"Óvalo\", \"Jirón\", \"Calle\", \"Malecón\", \"Pasaje\", \"Alameda\", \"Carretera\", \n",
    "                     \"Pasillo\", \"Prolongación\", \"Parque\", \"Camino\", \"Paseo\", \"Plaza\"]\n",
    "\n",
    "street_type_re = re.compile(r'^\\w+', re.IGNORECASE)\n",
    "\n",
    "def audit_street_types(street_types, street_name):\n",
    "    m = street_type_re.search(street_name)\n",
    "    if m:\n",
    "        street_type = m.group()\n",
    "        if street_type not in expected_st_names:\n",
    "            street_types[street_name]+=1\n",
    "    \n",
    "def get_street_types(filename):\n",
    "    street_types = defaultdict(int)\n",
    "    for _, elem in ET.iterparse(filename):\n",
    "        if elem.tag == \"tag\" and elem.attrib['k'] == 'addr:street':\n",
    "            audit_street_types(street_types, elem.attrib['v'])\n",
    "            \n",
    "        elem.clear()\n",
    "    return street_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1633\n"
     ]
    }
   ],
   "source": [
    "st = get_street_types(FILE_PATH)\n",
    "print (len(st))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Auditing the list of ~1600 uncommon street names, we can conclude the following facts:\n",
    "- There are overabbreviated street names and lack of consistency. For instance, \"Avenida\" can be found as \"avenida\", \"Av.\" \"av.\" or \"AV.\" \n",
    "- There are some misspelling cases: \"Av.enida\", \"Avenia\", \"Avienda\", \"Abenida\" or \"Jirón\" vs \"Jiron\"\n",
    "- Lots of streets don't have a street type. This is actually quite common in Latin America, where the people just refer to the street by it's name. In a city as extensive as Lima, if there's two streets with the same name they distingish them using the district (represented in the tag k=\"addr:city\" in this dataset). \n",
    "\n",
    "This last issue doesn't have a programatically solution, at least with the data provided by OpenStreetMap. With a database that include the relationship between streets and street types in the city of Lima, we would be able to solve it, but this is beyond the current scope of this project. \n",
    "\n",
    "When cleaning the data we have to take care of:\n",
    "- Overabbreviated street names replaced by full length names.\n",
    "- Misspelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phone Numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def audit_phone_type(phone_types, phone_name):\n",
    "    phone_types.add(phone_name)\n",
    "\n",
    "def audit_phone(filename):\n",
    "    osm_file = open(filename, \"r\", encoding=\"utf8\")\n",
    "    phones_types = set()\n",
    "    for _, elem in ET.iterparse(filename, events=(\"start\",)):\n",
    "        if elem.tag == \"tag\" and elem.attrib['k'] == 'phone':\n",
    "            for tag in elem.iter(\"tag\"):\n",
    "                    audit_phone_type(phones_types, tag.attrib['v'])\n",
    "    osm_file.close()\n",
    "    return phones_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1870\n"
     ]
    }
   ],
   "source": [
    "ph_list = audit_phone(FILE_PATH)\n",
    "print(len(ph_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Phone numbers appear in different formats: with or without international code (0051, +51, 51), with or without local code for land lines ((01),01,1), with separations, with hyphens and so on. \n",
    "\n",
    "Our objective now is to keep the numbers as clean as possible, so when cleaning we are going to strip the phone numbers from all the different extra numbers and signs and leave **7 digits number** for land lines and **9 digits number** for mobile. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data cleaning and database creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the findings of our audit, we have prepared some python code to clean and transform the original XML data into different .csv files to be dumped into a SQL database and analyzed using sqlite3. \n",
    "\n",
    "The cleaning and transformation process can be found in **cleaning.py** , **street_cleaner.py** and **phone_cleaner.py** . The schema for the csv files is found in **schema.py**. Finally, the code for creating the database is in **sql_db_creator.py**.\n",
    "\n",
    "The file **sample_creator.py** was used to create a sample (sample.osm) to check that everything was running smoothly before cleaning the main dataset.    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Querying the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "db = sqlite3.connect(\"Lima.db\")\n",
    "c = db.cursor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview Statistics\n",
    "Once we are connected to our database, we are going to perform some queries to see what information can we find. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Diego Sanguinetti', 376830),\n",
      " ('BikeRoad', 126627),\n",
      " ('ovruni', 81177),\n",
      " ('Karloss89', 54196),\n",
      " ('Favio Duran', 34095),\n",
      " ('Te-Ika', 23589),\n",
      " ('Baconcrisp', 22418),\n",
      " ('johnarupire', 17473),\n",
      " ('JAAS', 16085),\n",
      " ('mcaquino', 15770)]\n"
     ]
    }
   ],
   "source": [
    "# Top 10 contributos\n",
    "query = '''select user, count(*) as num \n",
    "from (select user from nodes union all select user from ways) \n",
    "group by user \n",
    "order by num desc limit 10'''\n",
    "c.execute(query)\n",
    "rows = c.fetchall()\n",
    "pprint.pprint(rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Diego Sanguinetti is the top contributor in this map, with more than twice the contributions made by BikeRoad, on the second position. Descending in the top5, the contributions decrease rapidly, but they decline steadily from there on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('school', 5407),\n",
      " ('kindergarten', 5220),\n",
      " ('restaurant', 3885),\n",
      " ('pharmacy', 1863),\n",
      " ('parking', 1432),\n",
      " ('bank', 1044),\n",
      " ('place_of_worship', 840),\n",
      " ('dentist', 716),\n",
      " ('fuel', 593),\n",
      " ('marketplace', 587),\n",
      " ('clinic', 537),\n",
      " ('cafe', 437),\n",
      " ('fast_food', 375),\n",
      " ('veterinary', 284),\n",
      " ('drinking_water', 282),\n",
      " ('fountain', 271),\n",
      " ('college', 249),\n",
      " ('bench', 242),\n",
      " ('police', 232),\n",
      " ('hospital', 222)]\n"
     ]
    }
   ],
   "source": [
    "# Top 10 amenities\n",
    "query = '''select value, count(*) as num\n",
    "from (select * from nodes_tags union all select * from ways_tags)\n",
    "where key=\"amenity\"\n",
    "group by value\n",
    "order by num desc\n",
    "limit 20'''\n",
    "c.execute(query)\n",
    "rows = c.fetchall()\n",
    "pprint.pprint(rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of these 10 amenities, we are going to focus on **place of worship** and **religion**, so we can find more about the Peruvian culture. \n",
    "\n",
    "Other interesting queries might be exploring the banks, pharmarcies and marketplaces to find the top companies in those segments. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('christian', 728),\n",
      " ('muslim', 2),\n",
      " ('Mormon', 1),\n",
      " ('catholic', 1),\n",
      " ('católica', 1),\n",
      " ('cristiana', 1)]\n"
     ]
    }
   ],
   "source": [
    "#Exploring religion by unique data point\n",
    "query = '''SELECT a.value, COUNT(*) as num\n",
    "FROM (SELECT * FROM nodes_tags UNION ALL SELECT * FROM ways_tags) as a\n",
    "      JOIN\n",
    "     (SELECT DISTINCT(id) FROM nodes_tags WHERE nodes_tags.value='place_of_worship'\n",
    "      AND nodes_tags.id NOT IN\n",
    "                       (SELECT ways_nodes.node_id FROM ways_nodes\n",
    "                        JOIN\n",
    "                       (SELECT DISTINCT(id) FROM ways_tags WHERE ways_tags.value='place_of_worship') as b\n",
    "                       ON ways_nodes.id = b.id)\n",
    "      UNION ALL\n",
    "      SELECT DISTINCT(id) FROM ways_tags WHERE ways_tags.value='place_of_worship') as c\n",
    "      ON a.id=c.id\n",
    "WHERE a.key='religion'\n",
    "GROUP BY a.value\n",
    "ORDER BY num DESC'''\n",
    "c.execute(query )\n",
    "rows = c.fetchall()\n",
    "pprint.pprint(rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the data, from nearly all the 730 places of worship in Lima, only 2 are muslim and only 1 mormon. The editors didn't seem to want to deepen in the different branches of the christian religion, so 99% of the entries are marked as christian. \n",
    "\n",
    "However, even if Peru is fundamentally a christian country, it's not in the same proportion as the idea we could get from the data. According to the 2017 census, 90% of the population identified themselves as christian. But in my experience, it is not difficult to find other religions' places of worhisp just by walking down the street, so we suggest to take this data with a pinch of salt. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('chinese', 271),\n",
      " ('chicken', 104),\n",
      " ('regional', 85),\n",
      " ('pizza', 81),\n",
      " ('peruvian', 43),\n",
      " ('seafood', 43),\n",
      " ('fish', 26),\n",
      " ('italian', 17),\n",
      " ('sushi', 12),\n",
      " ('japanese', 11),\n",
      " ('sandwich', 11),\n",
      " ('vegetarian', 11),\n",
      " ('fish_and_chips', 8),\n",
      " ('local', 8),\n",
      " ('burger', 7),\n",
      " ('fish;seafood', 5),\n",
      " ('grill', 5),\n",
      " ('korean', 5),\n",
      " ('vegan;vegetarian', 5),\n",
      " ('chinese;peruvian', 4),\n",
      " ('local;peruvian', 4),\n",
      " ('steak_house', 4),\n",
      " ('asian', 3),\n",
      " ('chicken;grill', 3),\n",
      " ('chinesse', 3)]\n"
     ]
    }
   ],
   "source": [
    "# Exploring cuisines\n",
    "query = '''select a.value, count(*) as num\n",
    "from (select * from nodes_tags union all select * from ways_tags) as a\n",
    "      join\n",
    "     (select distinct(id) from nodes_tags where nodes_tags.value='restaurant'\n",
    "      and nodes_tags.id not in\n",
    "                       (select ways_nodes.node_id from ways_nodes\n",
    "                        join\n",
    "                       (select distinct(id) from ways_tags where ways_tags.value='restaurant') as b\n",
    "                       on ways_nodes.id = b.id)\n",
    "      union all\n",
    "      select distinct(id) from ways_tags where ways_tags.value='restaurant') as c\n",
    "      on a.id=c.id\n",
    "where a.key='cuisine'\n",
    "group by a.value\n",
    "order by num desc\n",
    "limit 25'''\n",
    "c.execute(query)\n",
    "rows = c.fetchall()\n",
    "pprint.pprint(rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Peruvian food is a mix of local indigenous cuisines, European and Asiatic. This mix is so ingrained in their culture that even \"fried rice\" is called \"chaufa\", transcription from the Mandarin \"炒饭\"/\"Chǎo fàn\". Hence, it's not surprising to find Chinese restaurants in the most commons, but it is finding them in the first positions and with clear distance from the second (chicken). This might be fruit of some misconceptions, like trying to tag a restaurant as Chinese just by serving fried rice. In any case, it shows the importance that chicken and rice have for the Peruvian day to day diet. \n",
    "\n",
    "If we go deeper and don't limit the number of results displayed, we could see as the values in \"cuisine\" are utterly chaotic. There's not only present problems similar to what we found auditing the streets (like misspelling), but seems like the field was totally open and not with restricted parameters. This way, we can find entries like this:\n",
    "\n",
    "     ('ice_cream;burger;pasta;fine_dining;sandwich;regional;peruvian;mediterranean;italian;french;coffee_shop')\"\n",
    "\n",
    "It is specially common in Peru to find Nikkei restaurants where two or more cuisines are entangled with delicious results; but how to classify these restaurants? A possible solution to this problem could be a table structured database where more than one predetermined value (but limited up to three or four) can be added to each cuisine element. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion and other ideas about the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this dataset it has been seen as the nature of these collaborative tools where users can upload and update data has important drawbacks, like inconsistencies and errors in the data. It is, however, fruit of the trade-off between flexibility and cleanliness in a global tool like this one. \n",
    "\n",
    "Restricting the fields and adapting it to each country could be difficult for the people at OpenStreetMaps. However, this can also be seen as an opportunity for improving the database by using machine learning methods with existing data and create new restrictive fields by country; increasing this way the general uniformity.\n",
    "\n",
    "Something simpler to improve the uniformity of the data could be to follow the E.123 standards-based recommendation by the International Telecommunications Union to avoid having people writing special characters in the phone numbers field. This has two main benefits, since it would give a uniform treatment to all phone numbers and would avoid wrong numbers in the future. "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
